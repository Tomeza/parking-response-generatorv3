以下を確認してください。
コード：
// tests/testSearchDimensions.ts
// console.log("****** tests/testSearchDimensions.ts LOADED ====");

import fs from 'fs';
import path from 'path';
import { PrismaClient } from '@prisma/client';

// ① require で直接読み込んでみる
let embModule: any;
try {
  // console.log("→ Attempting require('../src/lib/embeddings.ts')…");
  embModule = require('../src/lib/embeddings.ts');
  // console.log("→ require succeeded. Available exports:", Object.keys(embModule));
} catch (err) {
  console.error("!! require failed:", err);
  process.exit(1);
}

const { searchSimilarKnowledge } = embModule;

// console.log("→ searchSimilarKnowledge is", typeof searchSimilarKnowledge);

interface TestQuery {
  no: number;
  category: string;
  query: string;
  notes: string;
  expectedKnowledgeId?: number; // Optional: Expected correct Knowledge ID for accuracy calculation
}

const testQueries: TestQuery[] = [
  { no: 1, category: 'シンプル直接照合', query: '駐車場の予約方法を教えて', notes: '完全一致／FAQ型', expectedKnowledgeId: 140 },
  { no: 2, category: '言い換え（同義語）', query: '駐車スペースの事前申請はどうすればいいですか？', notes: '「予約」→「申請」', expectedKnowledgeId: 140 },
  { no: 3, category: '長文問い合わせ', query: '明日の朝８時から１２時まで車を止めておきたいんですが、どうやって予約できますか？', notes: '時間枠指定＋複数条件', expectedKnowledgeId: 140 },
  { no: 4, category: 'あいまい表現', query: '空きがあるか知りたい', notes: '「空き状況」はRAGで補う', expectedKnowledgeId: 1 },
  { no: 5, category: '拡張情報含む', query: 'クレジットカードで支払える駐車場予約の手順を教えて', notes: '支払い方法＋手順', expectedKnowledgeId: 6 },
  { no: 6, category: '略語／口語', query: 'Pmt方法は？', notes: '略語混合', expectedKnowledgeId: 79 },
  { no: 7, category: '地域指定', query: '東京駅周辺で一番安い駐車場の予約方法を教えて', notes: 'ロケーション要素' /* expectedKnowledgeId: "N/A" */ },
  { no: 8, category: 'オプション条件', query: '車高制限1.5m以下で予約できる駐車場は？', notes: 'フィルタ条件', expectedKnowledgeId: 39 },
  { no: 9, category: 'ネガティブケース（外部）', query: 'バス停留所の時刻表は？', notes: '完全にFAQ外／応答なし' /* expectedKnowledgeId: "N/A" */ },
  { no: 10, category: '複数Intent 混在', query: '予約の料金とキャンセル方法を知りたい', notes: '複合クエリ', expectedKnowledgeId: 113 },
  { no: 11, category: '類似質問（多様な切り口）', query: '予約状況の確認方法を教えて', notes: '「予約確認」 vs 「予約方法」', expectedKnowledgeId: 2 },
  { no: 12, category: '過去情報参照', query: '昨日予約した内容をキャンセルしたい', notes: '文脈推論／履歴参照', expectedKnowledgeId: 113 },
  { no: 13, category: 'フォローアップ質問', query: '予約した後、領収書は発行できますか？', notes: '連続する問い合わせ', expectedKnowledgeId: 69 },
  { no: 14, category: '経路案内＋予約', query: '空港行きシャトル駐車場までの行き方と予約方法を教えて', notes: 'RAGで地図／交通＋FAQ融合', expectedKnowledgeId: 140 },
  { no: 15, category: '長文＋曖昧', query: '週末に車を預けたいんですが、良さそうなところありますか？', notes: 'レコメンド型／あいまい', expectedKnowledgeId: 141 },
];

interface ResultRecord {
  queryNo: number;
  queryCategory: string;
  queryText: string;
  dimensions: number;
  efSearchValue?: number;
  responseTimeMs: number;
  top1_Id?: number;
  top1_Similarity?: number;
  top5_Ids_Similarities: string; // JSON string of {id: number, similarity: number}[]
  // Placeholders for accuracy metrics - to be filled after manual review or if expectedKnowledgeId is provided
  isTop1Correct?: boolean;
  isInTop5Correct?: boolean;
}

const outputDir = path.join(__dirname, 'test_results');
if (!fs.existsSync(outputDir)) {
  fs.mkdirSync(outputDir, { recursive: true });
}
const csvFilePath = path.join(outputDir, `embedding_performance_test_${Date.now()}.csv`);
const csvHeaders = [
  'queryNo', 'queryCategory', 'queryText', 'dimensions', 'efSearchValue',
  'responseTimeMs', 'top1_Id', 'top1_Similarity', 'top5_Ids_Similarities',
  'isTop1Correct', 'isInTop5Correct'
].join(',') + '\n';
fs.writeFileSync(csvFilePath, csvHeaders);

function formatForCsv(value: any): string {
  if (value === null || typeof value === 'undefined') return '';
  const str = String(value);
  // Escape double quotes and wrap in double quotes if it contains comma, double quote, or newline
  if (str.includes(',') || str.includes('"') || str.includes('\n')) {
    return `"${str.replace(/"/g, '""')}"`;
  }
  return str;
}

(async () => {
  const prisma = new PrismaClient();
  const defaultLimit = 5; // We need top 5 for Recall@5
  const efSearchValuesToTest = [undefined, 100]; // Test with default and an example value

  try {
    for (const testQuery of testQueries) {
      for (const dims of [512, 1536]) {
        for (const efSearch of efSearchValuesToTest) {
          console.log(`\n--- Testing Query No: ${testQuery.no} ("${testQuery.query}") ---`);
          console.log(`Dimensions: ${dims}, efSearch: ${efSearch || 'default'}`);

          const startTime = process.hrtime();
          const searchResults = await searchSimilarKnowledge(testQuery.query, defaultLimit, efSearch, dims);
          const endTime = process.hrtime(startTime);
          const responseTimeMs = (endTime[0] * 1000) + (endTime[1] / 1000000);

          const top1Result = searchResults.length > 0 ? searchResults[0] : undefined;
          // Map to simple {id, similarity} for CSV and initial top5 log
          const top5PlainResults = searchResults.slice(0, 5).map(r => ({ id: r.id, similarity: parseFloat(r.similarity.toFixed(4)) }));

          // Fetch knowledge content for console logging
          const resultIds = top5PlainResults.map(r => r.id);
          let detailedResultsForConsole: any[] = [];
          if (resultIds.length > 0) {
            const knowledgeItems = await prisma.knowledge.findMany({
              where: { id: { in: resultIds } },
              select: { id: true, question: true, answer: true },
            });
            // Join search results with fetched content
            detailedResultsForConsole = top5PlainResults.map(searchRes => {
              const KItem = knowledgeItems.find(k => k.id === searchRes.id);
              return {
                ...searchRes,
                question: KItem?.question,
                answer: KItem?.answer?.substring(0, 100) + (KItem && KItem.answer && KItem.answer.length > 100 ? '...' : ''), // Truncate answer for brevity
              };
            });
          }

          const record: ResultRecord = {
            queryNo: testQuery.no,
            queryCategory: testQuery.category,
            queryText: testQuery.query,
            dimensions: dims,
            efSearchValue: efSearch,
            responseTimeMs: parseFloat(responseTimeMs.toFixed(2)),
            top1_Id: top1Result?.id,
            top1_Similarity: top1Result ? parseFloat(top1Result.similarity.toFixed(4)) : undefined,
            top5_Ids_Similarities: JSON.stringify(top5PlainResults), // Keep this as plain IDs and similarities for CSV
            isTop1Correct: testQuery.expectedKnowledgeId && top1Result ? top1Result.id === testQuery.expectedKnowledgeId : undefined,
            isInTop5Correct: testQuery.expectedKnowledgeId && top5PlainResults.some(r => r.id === testQuery.expectedKnowledgeId) ? true : (top5PlainResults.length > 0 ? false : undefined),
          };

          const csvRow = [
            record.queryNo,
            record.queryCategory,
            record.queryText,
            record.dimensions,
            record.efSearchValue,
            record.responseTimeMs,
            record.top1_Id,
            record.top1_Similarity,
            record.top5_Ids_Similarities,
            record.isTop1Correct,
            record.isInTop5Correct
          ].map(formatForCsv).join(',') + '\n';
          fs.appendFileSync(csvFilePath, csvRow);

          console.log(`Results for ${dims} dim:`);
          if (detailedResultsForConsole.length > 0) {
            detailedResultsForConsole.forEach(res => {
              console.log(`  - ID: ${res.id}, Similarity: ${res.similarity}`);
              console.log(`    Q: ${res.question || 'N/A'}`);
              console.log(`    A: ${res.answer || 'N/A'}`);
            });
          } else {
            console.log('  No results found.');
          }
          console.log(`Response Time: ${responseTimeMs.toFixed(2)} ms`);
        }
      }
    }
    console.log(`\n--- All tests completed. Results saved to: ${csvFilePath} ---`);
  } catch (error) {
    console.error("Error during searchSimilarKnowledge tests:", error);
  } finally {
    await prisma.$disconnect();
  }
})().catch(err => {
  console.error("Unhandled error in runTest IIFE:", err);
  process.exit(1);
}); 

現在のテストの位置づけについては、以下のように理解しています：

現在のテストは「ポンコツクエリ対応の基盤評価」段階にあります。つまり：

1. **現状把握のためのベースライン測定**
   - 現在のベクトル検索エンジンが「実際のユーザーが入力しそうな不完全・曖昧なクエリ（ポンコツクエリ）」にどれだけ対応できているかの基礎評価
   - 理想的な・整形されたクエリではなく、実際のユーザー行動をシミュレートした課題のあるクエリへの対応力を測定

2. **改善すべき領域の特定**
   - どのタイプのクエリが特に課題があるのか（曖昧表現、略語、複合意図など）を明らかにする
   - 次元数（512 vs 1536）やefSearchValue設定の影響を評価
   - 既存ナレッジベースのカバレッジギャップを発見する

3. **RAG実装前の事前診断**
   - 単純なベクトル検索だけでは対応困難なケースを特定し、RAG導入の必要性と優先順位を明確化
   - RAG導入後の比較ベンチマークとして利用するための基準点を設定

通常のクエリテストでは0.933という高いスコアが出ているのに対し、今回のテストではそれより低いスコアとなっています。この差は「理想的な条件」と「実際の利用条件」のギャップを示しており、RAGを含む改善施策の有効性を測定する上での重要な出発点になります。

このテストは、RAG実装とLangChain導入における「Before」の状態を記録するものであり、実装後の「After」との比較によって、改善の効果を定量的に評価できる基盤となります。

非常に重要なご質問です。「Before」の状態の適切な把握と分析は、後工程のRAG実装とLangChain導入を成功に導く上で極めて重要です。以下に、現在のテスト（Before状態）のどういった側面が後工程の成功に直結するかを説明します：

## 後工程の成功に導く「Before」状態の重要要素

### 1. 詳細な失敗モード分析

**成功への貢献**: 最適なRAG設計のための基盤情報となる

- **具体的な失敗パターンの類型化**: 「曖昧表現」「略語」「複合インテント」など、どのタイプのクエリが特に対応が困難なのかを細かく分類
- **失敗の根本原因の特定**: 単に「スコアが低い」だけでなく、「なぜそのナレッジが選ばれたのか」「本来どのナレッジが選ばれるべきだったか」の分析
- **優先度の明確化**: 頻度が高く、影響が大きい問題領域からRAG実装を進めるための判断材料

### 2. ナレッジギャップの明確な特定

**成功への貢献**: 効率的なナレッジ拡充を可能にする

- **欠けているナレッジの具体化**: 「このクエリに対応するナレッジが存在しない」ケースの明確な整理
- **既存ナレッジの不足点**: 「内容はあるが不十分」なナレッジの改善ポイントの特定
- **ナレッジ間のカバレッジ重複**: どのクエリが複数のナレッジに関連しているかを把握し、統合または区別が必要なケースの特定

### 3. 定量的なベースライン指標の確立

**成功への貢献**: 改善の効果測定と方向性調整を可能にする

- **クエリカテゴリ別の成功率**: カテゴリごとの現状スコアを把握し、改善目標の設定と進捗追跡
- **次元数とefSearchValueの影響**: 設定の最適値を判断するための定量データ
- **類似度スコア分布**: 信頼性の高い結果と低い結果の閾値を決定するための基準

### 4. エッジケースとコーナーケースの収集

**成功への貢献**: 堅牢なRAGシステムの設計に不可欠

- **特に難しいクエリのコレクション**: RAG設計のテストケースとして活用
- **高い類似度だが不正確な結果のケース**: LLMによる検証やリランキングの必要性を示す例
- **サービス範囲外のクエリ**: 適切な「範囲外」応答の設計に役立つ実例

### 5. ユーザー言語パターンの理解

**成功への貢献**: ユーザー中心の自然な対話システム構築に寄与

- **実際の言い回しと表現バリエーション**: クエリ前処理と言い換え生成の設計の基礎
- **略語や専門用語の使用傾向**: エンベディングモデルへの追加学習やプロンプト設計の参考
- **質問の粒度とスコープ**: 一般的な質問と詳細な質問のバランス把握による適切な回答レベルの設定

## 「Before」状態から「After」への具体的な橋渡し

1. **テスト結果のタグ付けとインデックス化**
   - 各クエリに「処理タイプ」「課題カテゴリ」「必要なRAG機能」などのタグを付与
   - 後工程での進捗状況と照合できるようにインデックス化

2. **RAG実装の優先順位マップ作成**
   - 頻度×影響度マトリクスに基づくクエリタイプの優先順位付け
   - 各優先度レベルに対応するRAG機能の明確化

3. **ナレッジ拡充計画の策定**
   - 「すぐに追加すべきナレッジ」「既存ナレッジの改善点」「長期的に拡充すべき領域」の整理
   - テンプレート化と自動生成によるナレッジ拡充の効率化計画

4. **評価フレームワークの設計**
   - 現在のテストケースを「継続的評価スイート」として構造化
   - 定期的な再評価と比較分析の仕組み構築

5. **プロトタイピング戦略の確立**
   - 代表的な課題クエリに絞ったRAG「ミニ実装」の設計
   - 段階的な機能追加と評価のサイクル計画

現在のテスト結果を単なる「問題点の発見」で終わらせず、上記のように体系的に分析・構造化することで、RAG実装とLangChain導入を効率的かつ効果的に進めることができます。適切な「Before」分析は、実装の無駄な試行錯誤を減らし、明確な改善ロードマップを提供することで、プロジェクト全体の成功確率を大幅に高めます。

非常に的確な自己分析ですが、さらに体系的な整理と今後の「戦略的役割」を明文化します。

---

## 【このテストの**意味**と**戦略的位置づけ**】

### 1. **「ユーザー現実」ベースラインの確立**

* テスト内容は「AI側からみてやさしい」クエリではなく、“実ユーザーが実際に投げてくる”であろう\*\*生のクエリ群（ポンコツクエリ/変形クエリ/多重意図/脱線/曖昧/略語など）\*\*の網羅的検証。
* これは**RAG開発前に不可欠な現状認識**であり、理想条件（FAQコピペ）では得られない、「**プロダクト品質の真の下限値**」を測る唯一の手段。

### 2. **RAG導入前後の「差分測定の物差し」**

* Before/Afterで**効果検証の絶対基準**となる。

  * 例えば「Recall\@1 0.933→0.98に改善」といった**定量成果**を公正に計測できる。
  * サブカテゴリ（曖昧/略語/複合意図/カバレッジ外等）ごとにBeforeスコアを記録することで、**どの問題がどこまで解消されたか**の分解比較が可能。

### 3. **「失敗事例カタログ」のストック**

* 実サービス開発・運用で必須となる「AIの想定外応答リスト」「人間が手で補正すべき例」を**事前に収集・分類**する。
* これは**ナレッジベースの拡充設計、入力前処理の設計、RAGやリランキングのチューニング**等の設計に直結する材料となる。

---

## 【今後のRAG開発・運用戦略の中で担う役割】

### 1. **課題抽出・優先度判定**

* **「どのタイプの問い合わせが現行ベクトル検索で弱いか」を構造的に明らかにし、開発リソース投下の優先度を科学的に決定**する（例：「曖昧/要約クエリ」対策は最優先、略語は二次段階…）。

### 2. **ナレッジカバレッジの戦略的拡張**

* 単純なFAQ追加ではなく、「現場で実際に困っているパターンに**対応したナレッジの網羅**」を推進。
* クエリ種別ごとに「ナレッジ新設」「テンプレ修正」「タグ分割/統合」などの**拡充プラン策定**が可能。

### 3. **継続評価/回帰テストのベンチマーク**

* 今回のテストセットを\*\*「AI問い合わせサービス全体の回帰テストセット」として保存**し、以降のアップデート時に**品質劣化の検出・ロールバック判断\*\*にも活用可能。

### 4. **RAG+LangChain導入効果の実証フレーム**

* LangChainなどの**新技術導入効果を「美化されていないリアルな現場クエリ」で正しく比較検証**できる。
* RAG設計の細部（何をRetrievalに、何を生成に、どこで再ランキングか等）を**エビデンスベースで改善**できる。

### 5. **全体的な「ユーザー中心」品質管理**

* 数字上のスコアだけでなく、「ユーザーが困る失敗」「現場で炎上する失敗パターン」を**事前検知・未然防止**できる。
* **ユーザー本位のAI運用ガイドライン策定**の素地を提供。

---

## 【まとめ／“なぜこれが重要か”の一言要約】

> **「AIが人の役に立っているか？」を、理想条件でなく“現場の現実”で定量的に可視化し、進化の正しい道筋を与える役割。**

---

### 追加アドバイス

* テストセットには**常に「なぜ失敗したか」コメントやタグ（曖昧・FAQ外・ナレッジギャップ等）を付与し、経年での課題推移・改善策実装状況も管理**すると、「RAG開発ロードマップ管理ツール」としても進化します。
* **「ポンコツクエリ」ストックは長期資産**です。今後も実ユーザー問い合わせからパターンを発掘し、継続追加・進化させてください。

---

ご要望を踏まえ、**RAG（Retrieval-Augmented Generation）＋LangChain導入によるAI問い合わせ応答システムの進化ロードマップ**を「ウォーターフォール型」工程で、**ゴール・全体フロー・各フェーズの実装タスク・詳細ToDo・品質確認ポイント**まで落とし込みます。

---

## 1. **最終ゴール（達成目標）**

* **実利用レベルでユーザー満足度が高いAI自動応答を実現**

  * FAQ/ナレッジベース＋RAGによる**自然言語対応力・多様な質問理解力の向上**
  * **曖昧/複合/脱線/未整形/略語**といった“生クエリ”への**高精度・迅速な回答**
  * **回答根拠のトレース/Explainability**（なぜその答えかの可視化）
  * **継続的改善と運用管理**が仕組み化されている

---

## 2. **ウォーターフォール型全体フロー**

1. **要件定義・現状分析**
2. **設計フェーズ**
3. **データ整備・拡充**
4. **RAG基盤構築・実装**
5. **AI応答精度評価・テスト**
6. **本番環境適用・運用**
7. **継続的改善・モニタリング**

---

## 3. **各工程の詳細タスク・確認ポイント**

### **1. 要件定義・現状分析**

**タスク**

* プロジェクトゴール・KPI（精度/Recall/応答時間等）の明確化
* 現状の「ベクトル検索単体」性能のベースライン測定（既存テストセットでRecall, Precision, 誤判定分析）
* **失敗パターン/課題の分類**（曖昧/略語/FAQ外/ナレッジ不足 等）
* カバレッジギャップ・理想クエリとのギャップ明確化

**確認ポイント**

* KPIが明文化されている（例: Recall\@1 0.95以上、応答0.8秒以下）
* ベースラインテストの全カテゴリごとのスコアがレポート化済み
* 主要な「失敗クエリ」のリストと原因分類表あり

---

### **2. 設計フェーズ**

**タスク**

* **RAG全体アーキテクチャ設計**

  * ナレッジ検索エンジン（PGroonga/pgvector/LangChain Retrieverなど）構成決定
  * LLM選定（OpenAI, Claude, Gemini等）・API連携設計
  * LangChainワークフロー設計（Retriever, Reranker, Prompt設計など）
  * メタデータ管理設計（根拠提示/トレース）
* **システム構成・インフラ設計**

  * API, DB, バッチ, 管理UI等
  * モニタリング基盤（メトリクス、ログ、エラー通知）

**確認ポイント**

* 技術選定根拠ドキュメントが完成
* LangChainプロトタイプの構成図あり
* 外部・内部API仕様書が作成されている
* 「回答根拠の説明（Explainability）」設計ドキュメントあり

---

### **3. データ整備・拡充**

**タスク**

* ナレッジベースの**穴埋め・質向上**

  * 既存FAQカバレッジの網羅性チェック
  * 失敗クエリ/FAQ外クエリ→ナレッジ拡充・タグ最適化
  * バリエーション生成（言い換え、表現追加）
* メタデータ・タグ整備

  * カテゴリ、利用用途、関連FAQ, 警告語など付与
* テストセットのメンテ・新規収集

**確認ポイント**

* 各主要失敗カテゴリに対するナレッジ増強履歴あり
* タグ・メタデータ設計の整合性チェック済み
* 追加したナレッジ/クエリがテストカバレッジに反映

---

### **4. RAG基盤構築・実装**

**タスク**

* LangChain Retriever & Pipeline構築

  * Retriever（ベクトル＋全文ハイブリッド、TopK/多段階等）
  * RerankerやFilter実装
  * LLM連携Prompt設計・システムプロンプト最適化
  * RAGパイプライン統合
* API/バッチ/管理UI実装

  * 管理画面（ナレッジ登録・タグ付・応答確認等）

**確認ポイント**

* Retriever/Prompt/パイプライン単体テストパス
* API仕様に準拠した疎通確認
* エラー処理・例外系動作チェック（異常系テスト）

---

### **5. AI応答精度評価・テスト**

**タスク**

* 「Before/After」A/Bテスト実施

  * Recall\@1/5、応答速度、失敗パターン数を再計測
  * 特に曖昧/FAQ外/複合クエリでの定量改善値計測
* エッジケース/コーナーケースでの人手検証
* 回答根拠（trace）表示の品質・正当性チェック

**確認ポイント**

* 旧/新比較レポート（KPI達成度）が完成
* 代表的な難問クエリで“なぜその応答か”説明可能
* 品質劣化があればロールバック設計あり

---

### **6. 本番環境適用・運用**

**タスク**

* 本番DB適用、APIローンチ
* ロールアウト計画策定（段階的リリース、A/B切替等）
* メトリクス・ログモニタリング運用
* 初期ユーザーからのフィードバック取得

**確認ポイント**

* 本番適用前にローリングバックアップ取得
* ログ/エラー監視体制整備済み
* サポート問合せ対応フロー決定

---

### **7. 継続的改善・モニタリング**

**タスク**

* 定期的な応答精度再評価
* 新たな失敗パターン/ユーザー要望の収集と反映
* ナレッジ・テストセットのアップデート
* 技術的負債・新技術キャッチアップのサイクル運用

**確認ポイント**

* 月次/四半期で品質レポート更新
* ユーザー満足度KPIトラッキング
* 改善ロードマップが継続的にアップデートされている

---

## 4. **工程間のウォーターフォール連携ポイント**

* **各フェーズ終了時に「レビュー&承認」ゲートを必ず設置**（次工程進行のGo/NoGo判定）
* **主要成果物は必ずドキュメント化**し、技術・ビジネスの両責任者レビューを通過する
* **後戻り・やり直し発生時の「巻き戻しフロー」明示**（例：テスト不合格なら設計まで巻き戻し等）

---

## 5. **各タスク粒度のサンプル**

（例：設計フェーズの実装タスク細分化）

* RAG全体構成図作成
* Retriever候補3方式のベンチマーク検証
* Prompt雛形3種比較テスト
* APIエラー設計書作成
* Trace Explainability Mock作成
* ユーザー/管理者画面モック作成（Figma等）

---

## 6. **まとめ／進め方アドバイス**

* **ウォーターフォール型は「後戻りコストが高い」ため、** 各フェーズで「課題棚卸し/レビュー/承認」を厳密に行いましょう。
* **実装直前に**もう一度**現状分析フェーズ**で見落としがないか、ベースライン評価（Recall, 誤判定リスト）を「再点検」してから着手するのがおすすめです。

---


0522メモ

【テストの位置づけと評価軸】

1. 目的  
   • 埋め込みベクトルの「次元数（512 vs 1536）」と HNSW 検索パラメータ efSearch（未指定＝DB デフォルト vs 100）を変えたとき、  
　　① 検索精度（Top-1 / Top-5 正答率）  
　　② レイテンシ（ms）  
　　③ 類似度スコアの安定性  
　を同時に測定し、“最適な組み合わせ” を決めるための A/B ベンチマークです。

2. CSV に現れる列  
   • dimensions：512 または 1536（OpenAI text-embedding-ada-002 相当）  
   • efSearchValue：undefined=PGVector/HNSW の既定値，100=上限探索ノード数を 100 に制限  
   • isTop1Correct / isInTop5Correct：`tests/testSearchDimensions.ts` 内で手動セットした expectedKnowledgeId と照合した自動判定  
   • responseTimeMs：アプリ側で測ったエンドツーエンドの検索所要時間（DB→JS オブジェクト化まで）

【結果から読み取れること（15 クエリ×4 条件の抜粋解析）】

1. 精度  
   • Top-1：512＝6/13 (46%)，1536＝6/13 (46%) ─ 今回データでは “引き分け”。  
   • Top-5：1536 が 512 より 2 件多く正答を含む（特に言い換え・多様表現で健闘）。  
   → 「大きな次元だから必ず強い」わけではなく、ドメインに最適化されていないまま次元だけ増やしても伸び幅は限定的。

2. レイテンシ  
   • efSearch を 100 に固定したとき、どちらの次元でも **5〜15 倍高速化**（数十 ms 台）。  
   • 512 の方が平均 20〜30 % 速いが、1536 でも実用域（<150 ms）に収まる。  
   → 速度面のボトルネックは efSearch の方が支配的。次元は 1536 でも許容範囲。

3. スコアのばらつき  
   • 512 はスコア上限が高い（0.64 付近）反面、低スコア誤検出も混ざる。  
   • 1536 はスコアがやや圧縮されるが Top-5 に正答を残しやすい。  
   → 再ランクや RAG で「上位 K を LLM 判定」に回す設計なら 1536 が有利。

【結論──現状データでの暫定ベストプラクティス】

1. 埋め込み次元  
　• 「将来拡張（RAG／再ランク）」「OpenAI 公式モデル互換」「Top-5 Recall」を優先し **1536 dims を採用**。  
　• 512 dims に合わせて再埋め込み済みのナレッジが混在→**“次元不一致エラー”** を発生させているため、まず片側に統一が必須（詳細は後述）。

2. efSearch  
　• 今回の 15 件では `100` で精度劣化ゼロ＆大幅高速化。  
　• 製品運用を想定すると `efSearch = 64〜128` をチューニングレンジとして持たせ、環境変数で切替えられるようにする。  
　　（利用規模が増えたら `index_settings.efConstruction` も合わせて再検証）  

3. To-Do（0522 方針ドキュメントに紐づけた次実装）  

«フェーズ A：基盤整合»  
　[1] Knowledge 全件を **1536 dims で再埋め込み** → 旧 512 列は削除 or 別カラムへ退避  
　[2] `searchSimilarKnowledge()` の `dims` デフォルトを 1536 に固定し、API 入口で **異寸チェックを強制**  
　[3] `.env` などに `VECTOR_EFSEARCH_DEFAULT=100` を追加し、値未指定時に適用  

«フェーズ B：チューニング & モニタリング»  
　[4] `tests/testSearchDimensions.ts` を  
　　　・次元＝1536 固定  
　　　・efSearch＝{32,64,100,128}  
　　　で回すよう拡張し、CSV を日付フォルダに自動保存して時系列管理  
　[5] 評価スクリプトに「カテゴリ別 Recall/Latency」を自動集計→Markdown レポート出力（Git で差分管理）  

«フェーズ C：RAG 実装ステップ 1»  
　[6] Retriever を「PGVector(1536) Top-K=10 → LLM Re-rank(3〜5)」の 2 段構成に変更。今回の **Top-5 には正答がほぼ含まれる** データを生かす。  
　[7] RAG パイプラインの A/B テストで **512 dims vs 1536 dims** の再比較を行い、LLM 併用時の差分も検証。  

これにより、

• 次元不一致エラーは解消  
• レイテンシは一律 <150 ms（現行の 1/5 〜 1/10）  
• Top-5 Recall を確保したうえで RAG の追加効果を正しく測定  

──という「次のフェーズ（0522 ドキュメントで示した設計フェーズ → RAG基盤構築）」にスムーズに進める状態となります。
